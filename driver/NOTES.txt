- To "bind" a driver to a backend device, the <device-id, pci-id> pair generated by QEMU backend (i.e. the device)
must be known to the driver while using MODULE_DEVICE_TABLE()

# STEP 1
/*
 * Read the config data for a PCI device, sanity-check it,
 * and fill in the dev structure.
 */
static struct pci_dev *pci_scan_device(struct pci_bus *bus, int devfn)
- Given the pci_bus struct, the function creates pci_dev struct for each 
device detected on the pci bus via pci_setup_device().


// probe.c
int pci_setup_device(struct pci_dev *dev): at boot time, pci_dev structs are initialized for 
every device. (virtio--simple is [1af4:10f3] here)

vm@cs695:~/virtio-simple$ sudo dmesg | grep class
[sudo] password for vm: 
[    0.639750] pci 0000:00:00.0: [8086:1237] type 00 class 0x060000 conventional PCI endpoint
[    0.640630] pci 0000:00:01.0: [8086:7000] type 00 class 0x060100 conventional PCI endpoint
[    0.641815] pci 0000:00:01.1: [8086:7010] type 00 class 0x010180 conventional PCI endpoint
[    0.657726] pci 0000:00:01.3: [8086:7113] type 00 class 0x068000 conventional PCI endpoint
[    0.660563] pci 0000:00:02.0: [1234:1111] type 00 class 0x030000 conventional PCI endpoint
[    0.701168] pci 0000:00:03.0: [1af4:1000] type 00 class 0x020000 conventional PCI endpoint
[    0.720691] pci 0000:00:04.0: [1af4:10f3] type 00 class 0x00ff00 conventional PCI endpoint
[    0.745245] pci 0000:00:05.0: [1af4:1001] type 00 class 0x010000 conventional PCI endpoint


# STEP 2
pci_device_add() -> int device_add(struct device *dev) : device_add creates the sysfs entries, device file etc.
pci_bus -> pci_dev
- The pci_dev struct contains pointer to driver, which for virtio-devices is set to the virtio-pci driver

# STEP 3
Look for a driver for this initialized device
bus_for_each_drv() : Each bus is assoc with a subsystem that has a set of loaded drivers (via bus_to_subsys())
__device_attach_driver() matches a driver based on vendor id and calls that driver's probe method. (aall virtio devices; using the 1af4 vendor id in table).

void bus_probe_device(struct device *dev) -> device_initial_probe -> __device_attach_driver -> call_driver_probe()
- This calls the probe method of the common pci driver. No virtio specific things are happening yet.
```
if (dev->bus->probe)
	ret = dev->bus->probe(dev);
```

# STEP 4
Call the COMMON virtio-pci probe function that is registed with the bus.

virtio_pci_common.c (driver virtio-pci)
/* Qumranet donated their vendor ID for devices 0x1000 thru 0x10FF. */
static const struct pci_device_id virtio_pci_id_table[] = {
	{ PCI_DEVICE(PCI_VENDOR_ID_REDHAT_QUMRANET, PCI_ANY_ID) },
	{ 0 }
};
a. 
 * Initialize device before it's used by a driver. Ask low-level code
 * to enable I/O and memory. Wake up the device if it was suspended.
 * Beware, this function can fail.
pci_enable_device()->do_pci_enable_device()

b. 
virtio_pci_modern_probe(): access the config space, capabilities etc.
```
notify = 	(pci_dev, VIRTIO_PCI_CAP_NOTIFY_CFG,
					IORESOURCE_IO | IORESOURCE_MEM,
					&mdev->modern_bars);
```
- This 	finds an mmio address to send notifications via `mdev->notify_base`
Each device virtqueue has its own notification area starting at mdev->notify_base.
The above function maps a large-ish chunk to ensure any number of virtqueues (and notifs)
can be supported by the pci device.

vp_modern_map_capability()
```
/**
 * pci_iomap_range - create a virtual mapping cookie for a PCI BAR
 * @dev: PCI device that owns the BAR
 * @bar: BAR number
 * @offset: map memory at the given offset in BAR
 * @maxlen: max length of the memory to map
 *
 * Using this function you will get a __iomem address to your device BAR.
 * You can access it using ioread*() and iowrite*(). These functions hide
 * the details if this is a MMIO or PIO address space and will just do what
 * you expect from them in the correct way.
 *
 * @maxlen specifies the maximum length to map. If you want to get access to
 * the complete BAR from offset to the end, pass %0 here.
 * */
- mapped via pci_iomap_range()
```
- this calls ioremap to return pointer to some area in a pci BAR.
- Note that in reality, the guest ends up mapping to an area in host heap that 
is allocated by QEMU.

c.
Assoicate a set of common virtio config ops with the virtio device
```
static const struct virtio_config_ops virtio_pci_config_ops = {
	.get		= vp_get,
	.set		= vp_set,
	.generation	= vp_generation,
	.get_status	= vp_get_status,
	.set_status	= vp_set_status,
	.reset		= vp_reset,
	.find_vqs	= vp_modern_find_vqs,
	.del_vqs	= vp_del_vqs,
	.synchronize_cbs = vp_synchronize_vectors,
	.get_features	= vp_get_features,
	.finalize_features = vp_finalize_features,
	.bus_name	= vp_bus_name,
	.set_vq_affinity = vp_set_vq_affinity,
	.get_vq_affinity = vp_get_vq_affinity,
	.get_shm_region  = vp_get_shm_region,
	.disable_vq_and_reset = vp_modern_disable_vq_and_reset,
	.enable_vq_after_reset = vp_modern_enable_vq_after_reset,
	.create_avq = vp_modern_create_avq,
	.destroy_avq = vp_modern_destroy_avq,
};
```

d.
Register the virtio device virtio_register_device(). Add the new virtio device to the "virtio" bus.
pci_bus (pci_device) -> virtio_bus (virtio_pci_device)

```
/*
	* device_add() causes the bus infrastructure to look for a matching
	* driver.
	*/
err = device_add(&dev->dev);
```

# STEP 5
Once the device specific driver is reg, the probe() is called (this is our simple-probe/etc)
- The virtio-pci funcs are thus made available to each virtio device via the config ops in 
struct virtio_pci_device.


## Interrupts (INTx, MSI, MSI-X)
https://docs.kernel.org/PCI/msi-howto.html#c.pci_irq_vector - API	

int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs, unsigned int max_vecs, unsigned int flags);
- Upon a successful allocation, the caller should use pci_irq_vector() to get the Linux IRQ number to be passed to request_threaded_irq(). 
- The driver must call pci_free_irq_vectors() on cleanup.

/proc/interrupts with virtio interrupt lines shows that device probe has been called successfully.


# STEP 6
Driver registers interrupt lines (MSI-X/MSI/INTx) for each virtqueue
used.
Enable virtqueue: iowrite16 to an ioremapped address that is interpreted as a 
control register, stored in struct virtio_pci_common_cfg
```
struct virtio_pci_common_cfg {
	/* About the whole device. */
	__le32 device_feature_select;	/* read-write */
	__le32 device_feature;		/* read-only */
	__le32 guest_feature_select;	/* read-write */
	__le32 guest_feature;		/* read-write */
	__le16 msix_config;		/* read-write */
	__le16 num_queues;		/* read-only */
	__u8 device_status;		/* read-write */
	__u8 config_generation;		/* read-only */

	/* About a specific virtqueue. */
	__le16 queue_select;		/* read-write */
	__le16 queue_size;		/* read-write, power of 2. */
	__le16 queue_msix_vector;	/* read-write */
	__le16 queue_enable;		/* read-write */
	__le16 queue_notify_off;	/* read-only */
	__le32 queue_desc_lo;		/* read-write */
	__le32 queue_desc_hi;		/* read-write */
	__le32 queue_avail_lo;		/* read-write */
	__le32 queue_avail_hi;		/* read-write */
	__le32 queue_used_lo;		/* read-write */
	__le32 queue_used_hi;		/* read-write */
};
```
This struct is part of an mdev i.e. virtio_pci_modern_device
- this struct has a pointer to a struct that contains ioremapped 
regions (i.e. the PCI device configuration space.)

struct msix_entry {
	u32	vector;	/* Kernel uses to write allocated vector */
	u16	entry;	/* Driver uses to specify entry, OS writes */
};


/**
 * msix_capability_init - configure device's MSI-X capability
 * @dev: pointer to the pci_dev data structure of MSI-X device function
 * @entries: pointer to an array of struct msix_entry entries
 * @nvec: number of @entries
 * @affd: Optional pointer to enable automatic affinity assignment
 *
 * Setup the MSI-X capability structure of device function with a
 * single MSI-X IRQ. A return of zero indicates the successful setup of
 * requested MSI-X entries with allocated IRQs or non-zero for otherwise.
 **/
static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
				int nvec, struct irq_affinity *affd)



/*
 * vp_modern_set_queue_enable - enable a virtqueue
 * @mdev: the modern virtio-pci device
 * @index: the queue index
 * @enable: whether the virtqueue is enable or not
 */
void vp_modern_set_queue_enable(struct virtio_pci_modern_device *mdev,
				u16 index, bool enable)
{
	vp_iowrite16(index, &mdev->common->queue_select);
	vp_iowrite16(enable, &mdev->common->queue_enable);
}


### PCI CORE auto-loads the driver module when the DEVICE_ID MATCH IS DETECTED
- note that the device id is not the standard device id, but a virtio-speicific device id 
see notes on qemu on host.

# Build instructions
1. make
2. compress via `zstd -z virtio-simple.ko -o virtio-simple.ko.zst`
3. `sudo cp virtio-simple.ko.zst /lib/modules/$(uname -r)/kernel/drivers/virtio/`
4. sudo depmod -a 
(above is for autoload module - depmod is essential)

# Device ioeventfd


### virtio drivers dont need to use pci_register_driver() because all the mappings are handled automatically by the 
virtio-pci core. 



## vq mapping between host and guest
```
AC
i know that findvqs in the driver adds msi-x interrupt lines visible in the /proc/interrupts.
If I want to send data to one virtqueue and receive response in the other, would the i would need to add *inbuf to one vq and *outbuf to the other vq. A kick on the outbuf (from driver) should cause the correct handler at the backend to be triggered. 
* The key idea is that we can have one vq just for outbufs (with corresp backend handler associated with that vq so that the correct callback is called.) right?
My query is : add_outbuf is done after device probe and find_vqs(). So how does the virtio subsystem know that tx_vq of the guest corresponds to the tx_vq of the host?
```
# find_vqs() in driver() will map to a host virtqueue in the order that the host virtqueues were defined.
This ensures that subsequent kicks to a vq by guest will reach the correct callback handler in qemu. (and vice versa)
--> This also establishes that for bidirectional data flows (req-response), 2 vqs minumum are neccessary.